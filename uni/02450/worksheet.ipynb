{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.499999999999998"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of model y=ax+b. Find w = [a,b]^T and compute prediction x=5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# setup model in standard format\n",
    "X = np.array([[1,1], [1,3], [1,4]]) # adding a ones-column to account for offset\n",
    "y = np.array([2,5,6])\n",
    "\n",
    "# compute weights\n",
    "w_star = np.linalg.inv(X.T @ X)@X.T@y\n",
    "\n",
    "# compute y\n",
    "input = [1,5]\n",
    "y = input@w_star\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Mining\n",
    "\n",
    "$support=\\frac{X}{\\lvert T \\rvert}$\n",
    "\n",
    "Support for an $item > 40\\%$ means $0.4 \\times \\lvert T \\rvert$ must be satisfied.\n",
    "\n",
    "$confidence=\\frac{support(X\\bigcup Y)}{support(X)}$\n",
    "\n",
    "### Apriori Algorithm\n",
    "Given new candidate itemssets $c$ in C for $L_k$.\n",
    "If $L_{k-1}$ does not contain a subset $s$ present in one of the $c$ itemsets. Then that $c$ is not considered.\n",
    "\n",
    "## Na√Øve Bayes Classifier\n",
    "\n",
    "$p\\left(y \\mid x_{1}, x_{2}, \\ldots, x_{M}\\right)=\\frac{p\\left(x_{1}, x_{2}, \\ldots, x_{M} \\mid y\\right) p(y)}{\\sum_{k=0}^{K-1} p\\left(x_{1}, x_{2}, \\ldots, x_{M} \\mid y=k\\right) p(y=k)}$\n",
    "\n",
    "## AUC\n",
    "\n",
    "$TPR / Recall / Sensitivity =\\frac{\\text { TP }}{\\text { TP }+\\text { FN }}$\n",
    "\n",
    "$Specificity=\\frac{\\text { TN }}{\\text { TN + FP }}$\n",
    "\n",
    "FPR = $1 - Specificity=\\frac{F P}{T N+F P}$\n",
    "\n",
    "Total number of positive examples: $TP + FN$\n",
    "\n",
    "Total number of negative examples: $FP + TN$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7500000000000001"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute AUC\n",
    "from sklearn import metrics\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 0, 0, 1])\n",
    "pred = np.array([5.7, 6.0, 6.2, 6.3, 6.4, 6.6, 6.7, 6.9, 7.0, 7.4])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr), thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.11],\n",
       "        [1.09],\n",
       "        [1.15]]),\n",
       " (4.890204457109357,\n",
       "  array([[      0.        , -520200.        , -460800.        ],\n",
       "         [-460800.        ,   -1800.        ,       0.        ],\n",
       "         [-520199.99999999,       0.        ,   -1800.        ]])),\n",
       " <bound method GaussianMixture._m_step of GaussianMixture(n_components=3, random_state=0)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "X = np.array([[2.11, 1.15, 1.09]]).T\n",
    "gm = GaussianMixture(n_components=3, random_state=0).fit(X)\n",
    "gm.means_, gm._e_step(X), gm._m_step\n",
    "\n",
    "#gm.predict([[0, 0], [12, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "x = np.linspace(0, 5, 10, endpoint=False)\n",
    "y = multivariate_normal.pdf(x, mean=2.5, cov=0.5)\n",
    "fig1 = plt.figure()\n",
    "ax = fig1.add_subplot(111)\n",
    "ax.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMC: -1.3333333333333333\n",
      "Jaccard: -5.166666666666667\n",
      "CosineSimilarity: [[-0.97590007]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15602/642619845.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.asarray(x, np.bool) # Not necessary, if you keep your data\n",
      "/tmp/ipykernel_15602/642619845.py:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.asarray(y, np.bool) # in a boolean array already!\n"
     ]
    }
   ],
   "source": [
    "r = np.array([[1,1,1,1,0,1]])\n",
    "s = np.array([[1,1,0,1,0,0]])\n",
    "\n",
    "def jaccard(x,y):\n",
    "  x = np.asarray(x, np.bool) # Not necessary, if you keep your data\n",
    "  y = np.asarray(y, np.bool) # in a boolean array already!\n",
    "  return np.double(np.bitwise_and(x, y).sum()) / x.size - np.double(np.bitwise_or(x, y).sum())\n",
    "\n",
    "def smc(x,y):\n",
    "  return np.double(np.bitwise_and(x,y).sum() + np.bitwise_not(x,y).sum()) / x.size\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"SMC: {smc(r,s)}\")\n",
    "print(f\"Jaccard: {jaccard(r,s)}\")\n",
    "print(f\"CosineSimilarity: {cosine_similarity(r,s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.92],\n",
       "        [6.12]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "x = np.array([[5.7, 6.0, 6.2, 6.3, 6.4, 6.6, 6.7, 6.9, 7.0, 7.4]]).T\n",
    "\n",
    "kmeans = KMeans(n_clusters=2).fit(x)\n",
    "#kmeans = KMeans(n_clusters=2, init=np.array([[x.mean]]).T).fit(x)\n",
    "\n",
    "kmeans.cluster_centers_, kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of NearestNeighbors(n_neighbors=3)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find k-nearest neighbours\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "samples = np.array([[5.7, 6.0, 6.2, 6.3, 6.4, 6.6, 6.7, 6.9, 7.0, 7.4]]).T\n",
    "neigh = NearestNeighbors(n_neighbors=3)\n",
    "neigh.fit(samples)\n",
    "neigh.kneighbors([[7.4]])\n",
    "\n",
    "# compute densities and average relative densities (modify K and arrays)\n",
    "K = 3\n",
    "(d1, d2, d3) = (1/K * np.array([0.1,0.3,0.4])).sum()**-1, (1/K*np.array([0.1,0.2,0.3])).sum()**-1, (1/K*np.array([0.1,0.2,0.3])).sum()**-1\n",
    "di = (1/K * np.array([0.4, 0.5, 0.7])).sum()**-1\n",
    "ard = di/(1/K*np.array([d1,d2,d3])).sum()\n",
    "ard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance explained\n",
    "def variance_explained(S, S_sub):\n",
    "    var_expl_pca = (S_sub**2).sum() / sum(S**2)\n",
    "    print(\"$Var_{Expl} Range$: \", var_expl_pca)\n",
    "\n",
    "S = np.array([14.14, 11.41, 9.46, 4.19, 0.17]) # s_i\n",
    "\n",
    "# modify range\n",
    "S_sub = S[:3]\n",
    "print(f\"Selected values: {S_sub}\")\n",
    "variance_explained(S, S_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "**Impurity Gain**\n",
    "$\\Delta=I(r)-\\sum_{k=1}^{K} \\frac{N\\left(v_{k}\\right)}{N(r)} I\\left(v_{k}\\right)$\n",
    "\n",
    "And then the 3 measure methods. See the image in this directory..\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017777777777777892"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Impurity gain using Gini\n",
    "I_r = 1-pow(5/15, 2)-pow(10/15, 2) # N(r) = 15\n",
    "I_1 = 1-pow(1/5, 2)-pow(4/5, 2) # N(v1) = 5\n",
    "I_2 = 1-pow(4/10, 2)-pow(6/10, 2) # N(v2) = 10\n",
    "\n",
    "Impurity_Gain = I_r-5/15*I_1-10/15*I_2\n",
    "Impurity_Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "error rate\n",
    "$\\epsilon_{t}=\\sum_{i=1}^{N} w_{i}(t)\\left(1-\\delta_{f_{t}\\left(\\boldsymbol{x}_{i}\\right), y_{i}}\\right)$ where $\\delta_{a, b}=\\left\\{\\begin{array}{ll}1 & \\text { if } a=b \\\\ 0 & \\text { if } a \\neq b\\end{array}\\right.$\n",
    "\n",
    "importance of classifier\n",
    "$\\alpha_{t}=\\frac{1}{2} \\log \\frac{1-\\epsilon_{t}}{\\epsilon_{t}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries (how to interpret)\n",
    "- Linear boundaries: Multinomial Regression\n",
    "- Smooth curvy boundaries: ANN\n",
    "- Axis aligned boundaries: Classification Tree\n",
    "- Complex non-smooth boundaries: 3-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
